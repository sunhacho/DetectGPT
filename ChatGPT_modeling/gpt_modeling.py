# -*- coding: utf-8 -*-
"""GPT_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ERgGKVi8D-FTC74FvSXshkfkHm53Bee
"""

import pandas as pd
df_combined=pd.read_csv('/content/drive/MyDrive/df_combined_l-5.csv')

pip install konlpy

import torch
from datasets import Dataset
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import pandas as pd
import re
from konlpy.tag import Okt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


# Okt 토크나이저 사용
okt = Okt()
# 불용어 리스트 (필요에 따라 수정)
stop_words = ['한', '에서', '하다', '를', '한다는', '이라는', '까지', '에서는', '으로', '인데', '이고', '에요', '이를', '있다', '여서', '이다',
              '부터', '지인', '짜다', '라고', '되었다', '고함', '고해']

def clean_text(text):
    # 텍스트가 문자열일 경우에만 처리
    if isinstance(text, str):
        # 특수 문자 제거
        text = text.lower()
        text = re.sub(r'[^a-zA-Z0-9가-힣\s]', '', text)
        return text
    return ''  # 텍스트가 문자열이 아니면 빈 문자열 반환

def tokenize_text(text):
    # 형태소 분석을 통한 토큰화
    return okt.morphs(text)

def sentence_length(text):
    return len(text.split())

def vocab_diversity(text):
    words = text.split()
    unique_words = set(words)
    return len(unique_words) / len(words) if len(words) > 0 else 0

def preprocess(df):
    df['cleaned_text'] = df['sentence'].apply(clean_text)  # 문장 정제
    df['tokens'] = df['cleaned_text'].apply(tokenize_text)  # 형태소 분석 후 토큰화
    # 품사 태깅 (명사, 동사, 형용사만 추출)
    df['filtered_tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words and len(word) > 1])
    # 특성 추가: 문장 길이, 어휘 다양성
    df['sentence_length'] = df['sentence'].apply(sentence_length)
    df['vocab_diversity'] = df['sentence'].apply(vocab_diversity)

    return df



# NaN 값이나 비어 있는 텍스트를 미리 처리
df_combined['sentence'] = df_combined['sentence'].fillna('')  # NaN을 빈 문자열로 대체

# 전처리
df_combined = preprocess(df_combined)

# 모델용 데이터셋 준비
train_df, test_df = train_test_split(df_combined[['sentence', 'label']], test_size=0.2, random_state=42)

# Hugging Face의 DistilBERT 토크나이저 불러오기
tokenizer = DistilBertTokenizerFast.from_pretrained("monologg/distilkobert")

# 토크나이저를 사용하여 텍스트를 인덱스로 변환
def tokenize_function(examples):
    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=512)

train_data = Dataset.from_pandas(train_df[['sentence', 'label']])
test_data = Dataset.from_pandas(test_df[['sentence', 'label']])

train_data = train_data.map(tokenize_function, batched=True)
test_data = test_data.map(tokenize_function, batched=True)

# 모델 불러오기
model = DistilBertForSequenceClassification.from_pretrained("monologg/distilkobert", num_labels=2)

# 훈련 인자 설정
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    save_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=10,
    weight_decay=0.01,
    report_to="none",
    load_best_model_at_end=True,  # 가장 좋은 모델 로드
    metric_for_best_model='eval_accuracy',  # 성능 지표로 사용할 것
    greater_is_better=True,  # 정확도가 높은 것이 더 좋음
    logging_steps=50,
    adam_epsilon=1e-8,
    warmup_steps=500,
)
# compute_metrics 함수
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = torch.argmax(torch.tensor(logits), axis=-1)
    accuracy = accuracy_score(labels, preds)
    return {"accuracy": accuracy}

# Trainer 설정
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    compute_metrics=compute_metrics,
)



# 모델 학습
trainer.train()

# 훈련이 끝난 후 가장 좋은 성능을 얻은 모델을 찾기
best_accuracy = 0
best_epoch = -1

# 각 epoch의 평가 정확도를 추적하고, 가장 좋은 정확도를 기록
for epoch in range(training_args.num_train_epochs):
    eval_result = trainer.evaluate(eval_dataset=test_data)
    eval_accuracy = eval_result['eval_accuracy']

    # 가장 높은 정확도를 기록하고 모델 저장
    if eval_accuracy > best_accuracy:
        best_accuracy = eval_accuracy
        best_epoch = epoch
        trainer.save_model(f'./best_model_epoch_{epoch}')  # 각 epoch마다 가장 좋은 모델 저장

# 훈련 후 최종 평가
trainer.evaluate()

# 예측 및 평가
predictions = trainer.predict(test_data)
predicted_labels = predictions.predictions.argmax(-1)  # 가장 높은 확률을 가진 클래스 선택
true_labels = test_data['label']

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 테스트 데이터에 대한 예측 생성
predictions = trainer.predict(test_data)
predicted_labels = predictions.predictions.argmax(-1)  # 가장 높은 확률을 가진 클래스 선택

# 실제 레이블
true_labels = test_data['label']  # 수정된 부분

# 정확도 계산
accuracy = accuracy_score(true_labels, predicted_labels)
print(f'Accuracy: {accuracy * 100:.2f}%')

# 성능 보고서 생성
report = classification_report(true_labels, predicted_labels, target_names=['Human', 'GPT'])  # 클래스 이름을 '인간', 'GPT'로 수정
print(report)

# 혼돈 행렬 계산 및 시각화
cm = confusion_matrix(true_labels, predicted_labels)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Human', 'GPT'], yticklabels=['Human', 'GPT'])  # 클래스 이름을 '인간', 'GPT'로 수정
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# 예측 확률 얻기 (predictions.predictions는 모든 클래스에 대한 확률을 포함)
probabilities = predictions.predictions
# positive 클래스에 대한 확률 추출 (두 번째 열)
positive_probabilities = probabilities[:, 1]

# ROC 곡선 계산
fpr, tpr, thresholds = roc_curve(true_labels, positive_probabilities)
roc_auc = auc(fpr, tpr)

# ROC 곡선 그리기
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# 모델 저장
model.save_pretrained('./saved_model')
tokenizer.save_pretrained('./saved_model')

pip install datasets

!pip install transformers

